#!/usr/bin/env bash
# ===============================================
# bootstrap.sh — Version 0.6 (WSL CUDA & AI Setup)
# -----------------------------------------------
# Author: Kevin Price (regenerated by ChatGPT)
# Purpose:
#   Configure a WSL Ubuntu environment for GPU-enabled
#   AI and development workloads (CUDA, Ollama, OpenWebUI).
#
# Notes:
#   - Idempotent: skips work that's already installed.
#   - Use AUTO=1 to run non-interactively (no pauses).
#   - Section pause prompts mirror the code comments.
# ===============================================

LOGFILE="$HOME/bootstrap.log"
exec > >(tee -a "$LOGFILE") 2>&1
set -e

# Make non-interactive if requested
export DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive}
# If user wants auditory/interactive pauses set AUTO=1 environment variable
AUTO=${AUTO:-}

pause() {
    # The caller should echo the section header before calling pause so message mirrors comment
    if [ -z "$AUTO" ]; then
        echo
        read -rp $'Press any key to continue to the next step... ' -n1 -s
        echo -e "\n"
    fi
}

echo "=== Starting Bootstrap Script v0.6 ==="
echo "Timestamp: $(date -u +"%Y-%m-%d %H:%M:%SZ")"
echo "Logfile: $LOGFILE"
echo "====================================="
echo "[INFO] Detected OS: $(lsb_release -ds 2>/dev/null || echo 'unknown')"
echo "[INFO] Kernel: $(uname -r)"
echo "====================================="

##############################################
# [0/10] Configure WSL default user and home
##############################################
echo "[0/10] Configure WSL default user and home (mirror: [0/10] Configure WSL default user and home)"
pause
if [ -f /etc/wsl.conf ]; then
    echo "[INFO] /etc/wsl.conf exists; backing up to /etc/wsl.conf.bak"
    sudo cp -n /etc/wsl.conf /etc/wsl.conf.bak || true
fi

sudo tee /etc/wsl.conf >/dev/null << 'EOF'
[user]
default=root

[boot]
command="cd ~"
EOF

##############################################
# [1/10] Update & Upgrade System
##############################################
echo "[1/10] Update & upgrade system packages (mirror: [1/10] Update & Upgrade System)"
pause
sudo apt-get update -y
sudo apt-get upgrade -y
sudo apt-get install -y --no-install-recommends software-properties-common wget curl ca-certificates gnupg apt-transport-https

##############################################
# [2/10] Install Fastfetch for system info
##############################################
echo "[2/10] Installing Fastfetch (mirror: [2/10] Install Fastfetch for system info)"
pause
if command -v fastfetch >/dev/null 2>&1; then
    echo "✔ Fastfetch already installed."
else
    if apt-cache show fastfetch >/dev/null 2>&1; then
        sudo apt-get install -y fastfetch
    else
        echo "⚠️ Fastfetch not found in default repos, installing latest GitHub .deb fallback..."
        tmpdeb="/tmp/fastfetch.deb"
        wget -q https://github.com/fastfetch-cli/fastfetch/releases/latest/download/fastfetch-linux-amd64.deb -O "$tmpdeb"
        sudo apt install -y "$tmpdeb" || { echo "❌ Installing fastfetch .deb failed"; rm -f "$tmpdeb"; }
        rm -f "$tmpdeb"
    fi
fi

# Add fastfetch + GPU query to .bashrc if not present
grep -q '^fastfetch$' ~/.bashrc || echo "fastfetch" >> ~/.bashrc
grep -q "nvidia-smi --query-gpu" ~/.bashrc || \
echo 'nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv,noheader' >> ~/.bashrc

##############################################
# [3/10] Install CUDA Toolkit & Nsight Systems
##############################################
echo "[3/10] Install CUDA Toolkit & Nsight Systems (mirror: [3/10] Install CUDA Toolkit (WSL-safe))"
pause

# Only try to install if nvcc is missing (idempotent)
if command -v nvcc >/dev/null 2>&1; then
    echo "✔ nvcc (CUDA compiler) already installed; skipping CUDA install."
else
    # Add NVIDIA CUDA keyring for Ubuntu 22.04 repository (works for WSL in most cases).
    echo "[INFO] Adding NVIDIA CUDA keyring and repository..."
    CUDA_REPO_BASE="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64"
    tmpkey="/tmp/cuda-keyring.deb"
    wget -q "${CUDA_REPO_BASE}/cuda-keyring_1.1-1_all.deb" -O "$tmpkey"
    sudo dpkg -i "$tmpkey" || true
    rm -f "$tmpkey"
    sudo apt-get update -y

    # Ensure libtinfo5 is present or create compatibility via libncurses5
    echo "[INFO] Ensuring libtinfo5 (Nsight dependency) is available..."
    if apt-cache policy libtinfo5 | grep -q "Candidate:"; then
        echo "✔ libtinfo5 available in repositories."
        sudo apt-get install -y libtinfo5 || echo "⚠️ failed to apt-get install libtinfo5"
    else
        echo "⚠️ libtinfo5 not available in repos. Applying compatibility shim using libncurses5..."
        # Install libncurses5, then symlink libtinfo.so.5 to the ncurses5 library if needed.
        sudo apt-get update -y
        sudo apt-get install -y libncurses5 || echo "⚠️ libncurses5 install failed; attempting alternate compatibility."

        # Attempt to find library paths and symlink
        LIB_DIRS=(/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu /lib /usr/lib)
        linked=false
        for d in "${LIB_DIRS[@]}"; do
            if [ -f "$d/libncurses.so.5" ] && [ ! -f "$d/libtinfo.so.5" ]; then
                echo "[INFO] Creating symlink: $d/libtinfo.so.5 -> $d/libncurses.so.5"
                sudo ln -sf "$d/libncurses.so.5" "$d/libtinfo.so.5" || true
                linked=true
            fi
        done

        if [ "$linked" = false ]; then
            echo "⚠️ Could not create libtinfo5 shim automatically. You may need to install libtinfo5 manually or add an older Ubuntu repo."
        else
            echo "✔ Compatibility shim for libtinfo5 created."
        fi
    fi

    # Install CUDA toolkit and attempt to install Nsight Systems (non-fatal if Nsight fails)
    echo "[INFO] Installing CUDA toolkit (may take a while)..."
    # Adjust toolkit package name if you prefer a different version (12-4, 12-6, etc.)
    sudo apt-get update -y
    sudo apt-get install -y cuda-toolkit-12-4 || sudo apt-get install -y cuda-toolkit-12-6 || { echo "⚠️ CUDA toolkit packages not found / install failed"; }

    # Nsight: try install but continue even if it fails (we already tried to fix libtinfo5)
    echo "[INFO] Installing Nsight Systems (best-effort)..."
    if apt-cache show nsight-systems-2023.4.4 >/dev/null 2>&1; then
        sudo apt-get install -y nsight-systems-2023.4.4 || echo "⚠️ Nsight Systems install failed (continuing)."
    else
        echo "⚠️ Nsight Systems package not available in apt cache; skipping apt install (you can install manually)."
    fi

    # Ensure cuda in PATH
    if ! grep -q "/usr/local/cuda/bin" ~/.bashrc 2>/dev/null; then
        echo 'export PATH=$PATH:/usr/local/cuda/bin' >> ~/.bashrc
    fi
    export PATH=$PATH:/usr/local/cuda/bin
fi

##############################################
# [4/10] Verify GPU Access
##############################################
echo "[4/10] Verify GPU access (mirror: [4/10] Verify GPU Access)"
pause
if command -v nvidia-smi >/dev/null 2>&1; then
    nvidia-smi || echo "⚠️ nvidia-smi returned an error (host/WSL driver may be needed)."
else
    echo "⚠️ nvidia-smi not found. This can be normal in WSL unless Windows GPU drivers & WSL GPU support are present."
fi

##############################################
# [5/10] Install system utilities (btop, etc.)
##############################################
echo "[5/10] Install system utilities (mirror: [5/10] Install system utilities (btop, etc.))"
pause
sudo apt-get install -y --no-install-recommends btop git curl

##############################################
# [6/10] Install Python, PyTorch (CUDA), HuggingFace
##############################################
echo "[6/10] Install Python, PyTorch (CUDA), and HuggingFace (mirror: [6/10] Install Python, PyTorch (CUDA), HuggingFace)"
pause
sudo apt-get install -y python3 python3-pip python3-venv
export PATH=$PATH:/usr/local/bin:~/.local/bin

if ! python3 -c "import torch" >/dev/null 2>&1; then
    # Attempt install with CUDA wheels; if fails, fallback to CPU-only install
    echo "[INFO] Installing PyTorch + extras (CUDA variant if available)..."
    pip install --break-system-packages torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 || \
    { echo "⚠️ CUDA-specific wheels failed; installing CPU-only torch"; pip install --break-system-packages torch torchvision torchaudio; }
    pip install --break-system-packages transformers accelerate sentencepiece || echo "⚠️ Some python packages failed to install"
else
    echo "✔ PyTorch already installed."
fi

python3 - << 'PYTEST' || true
try:
    import torch
    print("PyTorch CUDA available:", torch.cuda.is_available())
    if torch.cuda.is_available():
        print("GPU Name:", torch.cuda.get_device_name(0))
except Exception as e:
    print("PyTorch check failed:", e)
PYTEST

##############################################
# [7/10] Install and start Ollama
##############################################
echo "[7/10] Install and start Ollama (mirror: [7/10] Install and start Ollama)"
pause
if command -v ollama >/dev/null 2>&1; then
    echo "✔ Ollama CLI already present."
else
    echo "[INFO] Installing Ollama CLI..."
    curl -fsSL https://ollama.com/install.sh | sh || echo "⚠️ Ollama install script failed or requires manual steps."
fi

# Start Ollama if installed
if command -v ollama >/dev/null 2>&1; then
    echo "[INFO] Starting Ollama service..."
    pkill -f ollama 2>/dev/null || true
    nohup ollama serve > /var/log/ollama.log 2>&1 & disown || echo "⚠️ Could not background Ollama - check logs"
    sleep 5
    curl -s --max-time 3 http://localhost:11434/api/tags >/dev/null 2>&1 && echo "✔ Ollama API reachable" || echo "⚠️ Ollama API not reachable yet"
fi

##############################################
# [8/10] Install OpenWebUI
##############################################
echo "[8/10] Install OpenWebUI (mirror: [8/10] Install OpenWebUI)"
pause
if command -v open-webui >/dev/null 2>&1; then
    echo "✔ open-webui already installed."
else
    pip install --break-system-packages open-webui || echo "⚠️ pip install open-webui failed; check logs"
fi

# Ensure PATH updated
grep -q "/usr/local/bin:~/.local/bin" ~/.bashrc || echo 'export PATH=$PATH:/usr/local/bin:~/.local/bin' >> ~/.bashrc

echo "[INFO] Starting OpenWebUI on port 8080 (background)..."
pkill -f open-webui 2>/dev/null || true
nohup open-webui serve --host 0.0.0.0 --port 8080 --ollama-base-url http://localhost:11434 > /var/log/openwebui.log 2>&1 & disown || echo "⚠️ Could not background open-webui - check logs"
sleep 5

##############################################
# [9/10] Cleanup
##############################################
echo "[9/10] Cleanup (mirror: [9/10] Cleanup)"
pause
sudo apt-get autoremove -y
sudo apt-get clean

##############################################
# [10/10] Final Notes
##############################################
echo "[10/10] Finished (mirror: [10/10] Final Notes)"
pause
echo "==============================================="
echo "✅ Bootstrap completed (or attempted) — check $LOGFILE for details."
echo "✅ Access OpenWebUI at: http://<your-ip>:8080 (if running)."
echo "✅ Ollama API endpoint: http://localhost:11434 (if running)."
echo "✅ Run 'wsl --shutdown' from PowerShell to apply /etc/wsl.conf changes."
echo "==============================================="

# Exit successfully
exit 0
